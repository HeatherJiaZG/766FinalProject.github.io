<!doctype html>
<html>
<style>
  .container {
    display: flex;
    align-items: center;
    padding-left: 10px;
  }

  img {
    margin-left: 10px;
    margin-right: 10px;
  }
</style>


<head>
  <title>CS766 Project - Enhancing Image Resolution</title>
  <meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1">
  <link href="css/frame.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/controls.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/custom.css" media="screen" rel="stylesheet" type="text/css" />
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Open+Sans+Condensed:300,700' rel='stylesheet' type='text/css'>
  <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700" rel="stylesheet">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="js/menu.js"></script>
  <script>
    (function (i, s, o, g, r, a, m) {
      i['GoogleAnalyticsObject'] = r;
      i[r] = i[r] || function () {
        (i[r].q = i[r].q || []).push(arguments)
      }, i[r].l = 1 * new Date();
      a = s.createElement(o),
        m = s.getElementsByTagName(o)[0];
      a.async = 1;
      a.src = g;
      m.parentNode.insertBefore(a, m)
    })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');
    ga('create', 'UA-103598896-1', 'auto');
    ga('send', 'pageview');
  </script>
</head>

<body>
  <div class="menu-container"></div>
  <div class="content-container">
    <div class="content">
      <div class="content-table flex-column">
        <div class="flex-row">
          <div class="flex-item flex-column">
            <!-- <img class="image" id="me" src="img/me.jpeg"> -->
          </div>
          <div class="flex-item flex-column">
            <h2>Enhancing Image Resolution: A Comparative Study of Selected Deep Learning Approaches for Single-Image
              Super-Resolution</h2>
            <p class="text" align="center">
              Jingquan Wang (jwang2373)<br>
              Kunyang Li (kli253)<br>
              Heather Jia (yjia42)<br>
            </p>
          </div>
        </div>
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2>Background and Problem</h2>
            <hr>
            <p class="text">
              With the advent of AI, the problem of restoring more accurate brightness and realistic textures for
              low-resolution images has attracted much more attention. In this project, we plan to reimplement several
              popular deep learning approaches for 4x upscaling single-image super-resolution (SR), which refers to the
              challenging tasks of estimating a high-resolution image from its low-resolution counterpart. With nearly
              most of the methods proposed, there is still a clear gap between the model results and the ground truths.
              In spite of the breakthroughs in accuracy and speed of single image SR using faster and deeper
              convolutional neural networks, the major question remains unanswered: how to recover the finer texture
              details while super-resolving at high upscaling factors. More specifically, instead of only focusing on
              minimizing the mean square reconstruction error, researchers should also take high-frequency details of
              the images into consideration. For example, how to estimate photo-realistic natural images for 4x
              upscaling factors? What kind of model architecture and loss functions are more efficient? Attention on how
              to define a better and more efficient measurement for a more satisfying perceptual experience is needed
              for the problem.
            </p>
            <div align="center">
              <img src="img/intro1.png" width="650px" alt="srcnn chart"/> 
              <div class="img-caption" align="center">Low Resolution to High Resolution</div>
            </div>

            <p class="text">
              To enhance visual quality by generating plausible-looking natural images with high perceptual quality has
              become an attractive topic recently. This is large because it has been challenging to achieve before, but
              there are more promising directions with the arrival of AI, specifically machine learning algorithms,
              techniques – perceptual loss (i.e., optimize SR models in a feature space instead of pixel space),
              generative adversarial network (e.g., relativistic average GAN), etc. These advanced strategies can help
              bridge the gap between artificial images and natural ground truth. Three of the most representative ones
              published recently are SRCNN (super-resolution convolutional neural network), SRGAN (super-resolution
              generative adversarial network), and ESRGAN (enhanced super-resolution generative adversarial network). In
              our project, we plan to understand the underlying architecture and techniques, implement them, and further
              systematically compare their strengths and weaknesses. Besides, we will focus on different aspects of loss
              functions from the popular models, augment the loss function of SRCNN using techniques learned from other
              models to improve the performance.

            </p>



            <h3>Table of Content</h3>
            <ul>
              <li><a href="#importance">The Importance Of The Problem</a></li>
              <li><a href="#SOTA">State-Of-The-Art</a></li>
              <li><a href="#methodology">Methodology</a></li>
              <li><a href="#eval">Loss Function Investigation and Evaluation</a></li>
              <li><a href="#conclusion">Conclusion</a></li>
              <li><a href="#future">Future Work</a></li>
              <li><a href="#references">References</a></li>
              <li><a href="#tool">TBD</a></li>
            </ul>



            <h2 id="importance">The Importance Of The Problem</h2>
            <hr>
            <!--This list is reversed on the website due to reverse number listing-->
            <p class="text">
              The potential applications of the proposed project are numerous and exciting, with significant potential
              impact in various areas. People usually tend to use the highest quality possible for the videos they
              watch, indicating how visual quality is crucial in everyday life. It is also possible to “re-generate”
              popular anime produced long ago and bring the beloved classics back to the audience. The SR techniques can
              also be applied to cameras to make the blur objects far away from the camera much clearer with higher
              resolution. Besides, old photo recovery could also be an exciting field to apply SR. Old family photos
              from parents'/grandparents’ childhood could be restored with color and more details. It could also be
              applied to some historical pictures to make history more accessible and live to the public. The
              development of Computer Vision and SR approaches could push the state-of-the-art benchmark. Many proposed
              models exist, such as deeper networks with residual learning, Laplacian pyramid structure, and recursive
              learning.
            </p>



            <h2 id="SOTA">State-Of-The-Art</h2>
            <hr>
            <p class="text">
              Based on the Set14 dataset with a 4x upscaling task, the current state-of-the-art is the transformer-based
              neural network called HAT(Hybrid Attention Transformer), published in May 2022. Transformers have shown
              impressive performance on natural language processing tasks, and they have been applied to computer vision
              tasks nowadays, including image super-resolution tasks. However, we are not planning to implement HAT due
              to our lack of experience in transformers.
            </p>

            <p class="text">
              The key idea behind HAT is to activate more pixels in the transformer architecture by using an adaptive
              activation function. The authors observed that existing transformer-based super-resolution models tend to
              focus on a small set of pixels in the input image, resulting in a loss of high-frequency information. The
              adaptive activation function used in HAT addresses this issue by allowing the model to selectively
              activate more pixels in the input image based on their relevance to the final output. HAT also introduces
              a novel channel attention mechanism that enables the model to adaptively weigh each feature map's
              importance in the transformer encoder. This helps the model to focus on the most informative features and
              reduce the noise in the input image. The authors showed that HAT outperforms state-of-the-art
              transformer-based super-resolution models, achieving significant improvements in quantitative and
              qualitative measures.
            </p>

            <h2 id="Methodology">Methodology</h2>
            <hr>
            <h3 id="esrgan">SRCNN</h3>
            <h3 id="esrgan">SRGAN</h3>

            <h3 id="esrgan">ESRGAN</h3>
            <p class="text">
              Several critical problems of previous literature motivate the
              development of the Enhanced super-resolution generative
              adversarial networks, <a href="https://arxiv.org/abs/1809.00219">ESRGAN</a>.
              Super-resolution performance has been continuously improved via
              various network architecture designs and training strategies,
              especially the <a href="https://arxiv.org/abs/1511.04587">Peak
                Signal-to-Noise Ratio</a> (PSNR) value. PSNR is a typical and
              widely-used distribution measure to evaluate SR algorithms.
              However, these PSNR-oriented approaches during training tend to
              output over-smoothed results without sufficient high-frequency
              details. This is mainly because the PSNR metric fundamentally
              diverges from the subjective evaluation of human observers. In
              addition, ever since the pioneering work of SRCNN, deep
              convolutional neural network approaches have attracted broad
              attention from the field. But there is still a gap between both
              results of SRGAN and SRCNN and the ground-truth images. Aiming to
              bridge these gaps and better cover the quality of the images,
              ESRGAN builds on SRGAN by improving from three perspectives --
              network architecture, adversarial loss, and perceptual loss. A
              preview of an augmented image from our ESRGAN implementation is
              shown below.
            </p>
            <div class="container">
              <div class="image"> <img src="img/pic6.png" height="850px" /> </div>
              <div class="image"> <img src="img/pic6_ESRGAN.png" height="850px" /></div>
            </div>


            <h4 id="Network Architecture">Network Architecture</h4>
            <p class="text">
              ESRGAN introduces the Residual-in-Residual Dense Block as the
              basic network unit to help better retain information from the
              original iamge, which helps increase model capacity and reduce
              computational complexity. Besides, batch normalization in SRGAN is
              completely removed without accuracy sacrifice for better
              performance and generalization ability. Other several ligh-weight
              techniques are applied to facilitate the training process:
              residual scaling multiplies the residual components by a constant
              between 0 and 1 before adding them to the main path to scale down
              the effect of residuals for higher stability. Also, making the
              variance of the initial parameter lower speeds up the convergence.
            </p>

            <div class="container">
              <div class="image"> <img src="img/esrgan-architecture.png" height="850px" /> </div>
            </div>
            <div class="container">
              <div class="image"> <img src="img/esrgan-architecture2.png" height="850px" /> </div>
            </div>

            <h4 id="Adversarial loss">Adversarial Loss</h4>
            <p class="text">
              ESRGAN improves the discriminator by redesigning its adversarial
              losos with <a href="https://arxiv.org/abs/1807.00734">Relativistic
                average GAN</a> (RaGAN). Instead of evaluating "whether one image
              is real or fake", RaGAN focuses on "whether one image is mroe
              realistic than another". This shift of focus helps the generator
              to recover mroe realistic texture details of images by learning
              sharper edges and mroe detailed features.
            </p>
            <div class="container">
              <div class="image"> <img src="img/esrgan-ragan.png" height="850px" /> </div>
              <div class="image"> <img src="img/esrgan-ragan_code.png" height="850px" /> </div>
            </div>

            <h4 id="Perceptual Loss">Perceptual Loss</h4>
            <p class="text">
              ESRGAN uses VGG features before activation rather than after
              activation in SRGAN to enhance perceptual loss. This helps provide
              sharper edges and more visually pleasing results. Features after
              activation tend to be very sparse and, thus, can cause
              inconsistent reconstructed brightness compared with ground-truth
              images.

              Furthermore, ESRGAN designs a network interpolation strategy to
              balance perceptual quality and PSNR to remove noise in GAN-based
              methods while maintaining excellent perceptual quality. It first
              trains a PSNR-oriented network and then get a GAN-based network
              through fine-tuning. Then ESRGAN interpolates all parameters of
              these two networks correspondingly with a hyperparameter to derive
              an interpolated model below. This will help continuously balance
              perceptual quality and fidelity without re-training the model.
            </p>
            <div class="container">
              <div class="image"> <img src="img/esrgan-perceptual_loss.png" height="850px" /> </div>
            </div>

            <p class="text">
              Here are a few SR results enhanced by ESRGAN. (We used Set5 and
              Set14 for evaluation.)
            </p>

            <div class="container">
              <div class="image"> <img src="img/esrgan_res1.png" height="850px" /> </div>
            </div>
            <div class="container">
              <div class="image"> <img src="img/esrgan_res2.png" height="850px" /> </div>
            </div>



            <!--This list is reversed on the website due to reverse number listing-->
            
            <h2 id="eval">Loss Function Investigation and Evaluation</h2>
            <hr>
            <p class="text">
              Image super-resolution is by nature an ill-posed problem. With a single blurry image, 
              there is no evidence of the ground truth details. If the upscaling factor is too large,
               a great amount of information from the original high-resolution image could be lost. 
               The results of super-resolution are normally evaluated by peak signal-to-noise ratio 
               (PSNR) value. Therefore, most of the supervised algorithms that deal with super-resolution 
               make use of the mean squared error loss between the high-resolution image that is 
               acquired and the ground truth of the particular image. Minimizing MSE value automatically 
               maximizes PSNR. 
            </p>

            <p class="text">
              SRCNN was one of the first works which explored deep convolutional neural networks for 
              image super resolution. Although it is a very simple neural network that consists of 
              only three parts, it is yet very powerful in solving the single-image super-resolution 
              problem. It learns to map the low resolution images to the high resolution ones with little 
              pre or post processing. Its simple structure consists of patch extraction and representation, 
              non-linear mapping, and reconstruction. Its loss function is very powerful and effective
               in solving the problem. It adopts a standard loss function, which is the average of mean 
               square error for the training samples. It aims to minimize the MSE difference between the
                generated super-resolution image and the original ground truth high-resolution image via
                 back propagation. SRCNN proves to train quicker and achieve high-level accuracy.
            </p>

            <p class="text">
              However, PSNR value is more oriented towards finding the features of each individual pixel 
              and not more visually perceptive attributes such as the high texture detail of the particular
               picture. Some papers also evaluate the results with human perception scores. For example, 
               SRGAN uses a feature extraction network to get the embeddings of the generated SR image and 
               minimizes the difference between the embeddings of the ground truth HR image. This is 
               addressed as content loss. It focuses less on the pixel-by-pixel comparison of the images, 
               and is mostly concerned about the improvement in the overall quality of the images. The 
               lower resolutions fail to highlight some of the finer and critical details in the generated 
               picture, which is solved with an increase in the resolution and overall quality. They 
               prefer to consume most visualizations in the modern world in the highest quality so that 
               we as the audience and viewers can have the best experience from the particular content. 
               Similarly, ESRAN adopts a perceptual loss with minor modifications to the network architecture. 
            </p>

            <p class="text">
              With the improvements made by SRGAN and ESRGAN, we are thinking how content and MSE loss 
              functions might affect model performance for the single-image super-resolution problem with 
              a X4 upscaling factor. We modify the loss functions of SRCNN and SGRAN to explore how different 
              weights of two kinds of loss function components perform under CNN and GAN network architectures. 
              For evaluation metrics, we use PSNR, a typical and widely-used measure to evaluate SR algorithms, 
              and also human perception, which makes the evaluation more practical for real world use cases. 
              The PSNR evaluations are performed on Set5 and Set14 datasets, which are standard evaluation 
              datasets for the single-image super-resolution problem. 
            </p>

            <div align="center">
              <img src="img/srcnn/srcnn-chart.JPG" width="650px" alt="srcnn chart"/> 
              <div class="img-caption" align="center">SRCNN Evaluation Results</div>
            </div>
            
            <p class="text">
              For the content loss in our experiments, we are referring to comparing a pretrained VGG19 
              network output specifically. The only way that the real image VGG output and the fake image 
              VGG output will be similar is when the input images themselves are similar. The intuition 
              behind this is that pixel-wise comparison will help compound the core objective of achieving 
              super-resolution. In this section, we show how different weights of content loss and MSE loss
               affect the training results and training efforts on CNN and GAN models for the x4 downscaled 
               single-image super-resolution problem. 
            </p>

            <p class="text">
              The chart above shows the experiments on SRCNN. All experiments are trained 400 epochs on the 
              “91-image” dataset. Each image from the training dataset is cropped to 32*32 sub-images and 
              used as the input for the network. Each epoch contains over 24k sub-images, extracted from the 
              original images with a stride of 4. The middle column indicates the loss function used on the 
              CNN model, which is the only difference between these experiments that share the same network 
              architecture. For example, EXP5 is the original SRCNN and gets a PSNR value even better than 
              the original SRCNN paper. Vgg_loss is calculated by the L1 distance between the VGG output of 
              the generated image and the ground truth HR image. The last column is the PSNR evaluation result 
              on Set14 downsampled by 4. 
            </p>


            <div align="center">
              <img src="img/srcnn/srcnn-exp1.png" width="150px" alt="srcnn exp1"/> 
              <img src="img/srcnn/srcnn-exp2.png" width="150px" alt="srcnn exp2"/>
              <img src="img/srcnn/srcnn-exp3.png" width="150px" alt="srcnn exp3"/>
              <div class="img-caption" align="center">Outputs from SRCNN EXP1, EXP2, EXP3 (left to right)</div>
            </div>
            <div align="center">
              <img src="img/srcnn/srcnn-exp4.png" width="150px" alt="srcnn exp4"/>
              <img src="img/srcnn/srcnn-exp5.png" width="150px" alt="srcnn exp5"/>
              <img src="img/srcnn/srcnn-exp6.png" width="150px" alt="srcnn exp6"/>
              <div class="img-caption" align="center">Outputs from SRCNN EXP4, EXP5, EXP6 (left to right)</div>
            </div>

            <p class="text">
              We toggled the combination of MSE and VGG losses used for the network and ran the experiments 
              under the same configurations. As can be seen from this chart, increasing the weight of VGG 
              loss on SRCNN produces better PSNR results on Set14 (in EXP3). However, its generated images 
              look less smooth. Perceptually speaking, evaluation on Set14 shows that a higher weight of MSE 
              loss function results in a more smooth output image. While the images created with more VGG loss 
              involved show more contrast and details but are less smooth, which is further explained in SRGAN 
              experiments. As expected, EXP6 creates images that look very far from the original image. It is 
              partly because we used only one channel as the input to the VGG layer and thus it does not 
              extract color information. This is why the example image from EXP6 shows a skeleton of the 
              image but does not show correct color. It results in an extremely low PSNR evaluation result. 
              These experiments in the chart all reach their highest evaluation PSNR around the late 300s 
              epochs, which shows little evidence of training efficiency in different configurations. 
            </p>

            <div align="center">
              <img src="img/srgan/srgan-chart.JPG" width="650px" alt="srgan chart"/> 
              <div class="img-caption" align="center">SRGAN Evaluation Results</div>
            </div>

            <p class="text">
              SRGAN adopts a VGG loss as part of its generator loss function in the original setting. It uses 
              a perceptual loss, which is the weighted sum of two loss components: content loss and adversarial 
              loss. The content loss uses the euclidean distance between the VGG feature representations. The 
              adversarial loss generates highly realistic images that can help fool the discriminator. The 
              discriminator aims to train a generative model to fool a differentiable discriminator that is 
              trained to distinguish generated images from real images. As for SRGAN experiments, we also toggled 
              the combination of MSE and VGG losses used for the generator network and ran the experiments under 
              the same configurations. The training set is the entire CelebA dataset, which is a large-scale 
              face attributes dataset with more than 200K celebrity images. It takes a very long time to train 
              due to the limitations of our accessible machines. We trained each experiment in the chart above 
              for 10 epochs on CelebA. The middle column is the different configurations of the generator loss 
              function, and the rest of the settings remain identical for different experiments. The third 
              column is the PSNR evaluation result from Set14 with a downsample factor of x4. As can be seen 
              in the chart, EXP1’s PSNR is lower than EXP2’s, which shows adding a MSE component in generator’s 
              loss can achieve higher PSNR in the initial training epochs. As shown in these zebra pictures below, 
              if we heavily rely on perceptual loss, it shows more contrast and details of the texture, which is 
              not helpful to make the pictures look natural or smooth perceptually. However, it did help the 
              model learn the textures fast. 
            </p>
            <p class="text">
              
            </p>
           
            <h2 id="conclusion">Conclusion</h2>
            <hr>
            <p class="text">
              
            </p>
            
            
            <h2 id="future">Future Work</h2>
            <hr>
            <p class="text">
              
            </p>



            <h2 id="references">References</h2>
            <hr>
            
            
            <h2 id="tool">TBD</h2>
            <hr>
            
            
        </div>
      </div>
    </div>
  </div>
</body>

</html>
